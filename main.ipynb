{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "given below are the  modules which are downloaded  for the operations  as such  for the web scrapping to extract text and heading from the   websites which would be  then used here as per the developement we will be using three modules to  facilitates the operations which are beautifulSoup , scrappy and  selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\mehta\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.12.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\mehta\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from beautifulsoup4) (2.5)\n",
      "Collecting scrapy\n",
      "  Downloading Scrapy-2.11.2-py2.py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting Twisted>=18.9.0 (from scrapy)\n",
      "  Downloading twisted-24.7.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting cryptography>=36.0.0 (from scrapy)\n",
      "  Downloading cryptography-43.0.0-cp39-abi3-win_amd64.whl.metadata (5.4 kB)\n",
      "Collecting cssselect>=0.9.1 (from scrapy)\n",
      "  Downloading cssselect-1.2.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting itemloaders>=1.0.1 (from scrapy)\n",
      "  Downloading itemloaders-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting parsel>=1.5.0 (from scrapy)\n",
      "  Downloading parsel-1.9.1-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Collecting pyOpenSSL>=21.0.0 (from scrapy)\n",
      "  Downloading pyOpenSSL-24.2.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting queuelib>=1.4.2 (from scrapy)\n",
      "  Downloading queuelib-1.7.0-py2.py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting service-identity>=18.1.0 (from scrapy)\n",
      "  Downloading service_identity-24.1.0-py3-none-any.whl.metadata (4.8 kB)\n",
      "Collecting w3lib>=1.17.0 (from scrapy)\n",
      "  Downloading w3lib-2.2.1-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting zope.interface>=5.1.0 (from scrapy)\n",
      "  Downloading zope.interface-7.0.1-cp312-cp312-win_amd64.whl.metadata (44 kB)\n",
      "Collecting protego>=0.1.15 (from scrapy)\n",
      "  Downloading Protego-0.3.1-py2.py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting itemadapter>=0.1.0 (from scrapy)\n",
      "  Downloading itemadapter-0.9.0-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting setuptools (from scrapy)\n",
      "  Using cached setuptools-72.1.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\mehta\\appdata\\roaming\\python\\python312\\site-packages (from scrapy) (24.1)\n",
      "Collecting tldextract (from scrapy)\n",
      "  Downloading tldextract-5.1.2-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting lxml>=4.4.1 (from scrapy)\n",
      "  Downloading lxml-5.3.0-cp312-cp312-win_amd64.whl.metadata (3.9 kB)\n",
      "Collecting defusedxml>=0.7.1 (from scrapy)\n",
      "  Downloading defusedxml-0.7.1-py2.py3-none-any.whl.metadata (32 kB)\n",
      "Collecting PyDispatcher>=2.0.5 (from scrapy)\n",
      "  Downloading PyDispatcher-2.0.7-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting cffi>=1.12 (from cryptography>=36.0.0->scrapy)\n",
      "  Downloading cffi-1.17.0-cp312-cp312-win_amd64.whl.metadata (1.6 kB)\n",
      "Collecting jmespath>=0.9.5 (from itemloaders>=1.0.1->scrapy)\n",
      "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: attrs>=19.1.0 in c:\\users\\mehta\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from service-identity>=18.1.0->scrapy) (24.1.0)\n",
      "Collecting pyasn1 (from service-identity>=18.1.0->scrapy)\n",
      "  Downloading pyasn1-0.6.0-py2.py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting pyasn1-modules (from service-identity>=18.1.0->scrapy)\n",
      "  Downloading pyasn1_modules-0.4.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting automat>=0.8.0 (from Twisted>=18.9.0->scrapy)\n",
      "  Downloading Automat-22.10.0-py2.py3-none-any.whl.metadata (1.0 kB)\n",
      "Collecting constantly>=15.1 (from Twisted>=18.9.0->scrapy)\n",
      "  Downloading constantly-23.10.4-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting hyperlink>=17.1.1 (from Twisted>=18.9.0->scrapy)\n",
      "  Downloading hyperlink-21.0.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting incremental>=24.7.0 (from Twisted>=18.9.0->scrapy)\n",
      "  Downloading incremental-24.7.2-py3-none-any.whl.metadata (8.1 kB)\n",
      "Collecting typing-extensions>=4.2.0 (from Twisted>=18.9.0->scrapy)\n",
      "  Downloading typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: idna in c:\\users\\mehta\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tldextract->scrapy) (3.7)\n",
      "Requirement already satisfied: requests>=2.1.0 in c:\\users\\mehta\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tldextract->scrapy) (2.32.3)\n",
      "Collecting requests-file>=1.4 (from tldextract->scrapy)\n",
      "  Downloading requests_file-2.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting filelock>=3.0.8 (from tldextract->scrapy)\n",
      "  Downloading filelock-3.15.4-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: six in c:\\users\\mehta\\appdata\\roaming\\python\\python312\\site-packages (from automat>=0.8.0->Twisted>=18.9.0->scrapy) (1.16.0)\n",
      "Collecting pycparser (from cffi>=1.12->cryptography>=36.0.0->scrapy)\n",
      "  Downloading pycparser-2.22-py3-none-any.whl.metadata (943 bytes)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\mehta\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.1.0->tldextract->scrapy) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\mehta\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.1.0->tldextract->scrapy) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mehta\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.1.0->tldextract->scrapy) (2024.7.4)\n",
      "Downloading Scrapy-2.11.2-py2.py3-none-any.whl (290 kB)\n",
      "Downloading cryptography-43.0.0-cp39-abi3-win_amd64.whl (3.1 MB)\n",
      "   ---------------------------------------- 0.0/3.1 MB ? eta -:--:--\n",
      "   -------------------- ------------------- 1.6/3.1 MB 7.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.1/3.1 MB 7.2 MB/s eta 0:00:00\n",
      "Downloading cssselect-1.2.0-py2.py3-none-any.whl (18 kB)\n",
      "Downloading defusedxml-0.7.1-py2.py3-none-any.whl (25 kB)\n",
      "Downloading itemadapter-0.9.0-py3-none-any.whl (11 kB)\n",
      "Downloading itemloaders-1.3.1-py3-none-any.whl (12 kB)\n",
      "Downloading lxml-5.3.0-cp312-cp312-win_amd64.whl (3.8 MB)\n",
      "   ---------------------------------------- 0.0/3.8 MB ? eta -:--:--\n",
      "   ------------- -------------------------- 1.3/3.8 MB 6.7 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 3.1/3.8 MB 7.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.8/3.8 MB 7.1 MB/s eta 0:00:00\n",
      "Downloading parsel-1.9.1-py2.py3-none-any.whl (17 kB)\n",
      "Downloading Protego-0.3.1-py2.py3-none-any.whl (8.5 kB)\n",
      "Downloading PyDispatcher-2.0.7-py3-none-any.whl (12 kB)\n",
      "Downloading pyOpenSSL-24.2.1-py3-none-any.whl (58 kB)\n",
      "Downloading queuelib-1.7.0-py2.py3-none-any.whl (13 kB)\n",
      "Downloading service_identity-24.1.0-py3-none-any.whl (12 kB)\n",
      "Downloading twisted-24.7.0-py3-none-any.whl (3.2 MB)\n",
      "   ---------------------------------------- 0.0/3.2 MB ? eta -:--:--\n",
      "   ---------------- ----------------------- 1.3/3.2 MB 7.4 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 2.9/3.2 MB 7.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.2/3.2 MB 6.4 MB/s eta 0:00:00\n",
      "Downloading w3lib-2.2.1-py3-none-any.whl (21 kB)\n",
      "Downloading zope.interface-7.0.1-cp312-cp312-win_amd64.whl (211 kB)\n",
      "Using cached setuptools-72.1.0-py3-none-any.whl (2.3 MB)\n",
      "Downloading tldextract-5.1.2-py3-none-any.whl (97 kB)\n",
      "Downloading Automat-22.10.0-py2.py3-none-any.whl (26 kB)\n",
      "Downloading cffi-1.17.0-cp312-cp312-win_amd64.whl (181 kB)\n",
      "Downloading constantly-23.10.4-py3-none-any.whl (13 kB)\n",
      "Downloading filelock-3.15.4-py3-none-any.whl (16 kB)\n",
      "Downloading hyperlink-21.0.0-py2.py3-none-any.whl (74 kB)\n",
      "Downloading incremental-24.7.2-py3-none-any.whl (20 kB)\n",
      "Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Downloading requests_file-2.1.0-py2.py3-none-any.whl (4.2 kB)\n",
      "Downloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Downloading pyasn1-0.6.0-py2.py3-none-any.whl (85 kB)\n",
      "Downloading pyasn1_modules-0.4.0-py3-none-any.whl (181 kB)\n",
      "Downloading pycparser-2.22-py3-none-any.whl (117 kB)\n",
      "Installing collected packages: PyDispatcher, w3lib, typing-extensions, setuptools, queuelib, pycparser, pyasn1, protego, lxml, jmespath, itemadapter, hyperlink, filelock, defusedxml, cssselect, constantly, automat, zope.interface, requests-file, pyasn1-modules, parsel, incremental, cffi, Twisted, tldextract, itemloaders, cryptography, service-identity, pyOpenSSL, scrapy\n",
      "Successfully installed PyDispatcher-2.0.7 Twisted-24.7.0 automat-22.10.0 cffi-1.17.0 constantly-23.10.4 cryptography-43.0.0 cssselect-1.2.0 defusedxml-0.7.1 filelock-3.15.4 hyperlink-21.0.0 incremental-24.7.2 itemadapter-0.9.0 itemloaders-1.3.1 jmespath-1.0.1 lxml-5.3.0 parsel-1.9.1 protego-0.3.1 pyOpenSSL-24.2.1 pyasn1-0.6.0 pyasn1-modules-0.4.0 pycparser-2.22 queuelib-1.7.0 requests-file-2.1.0 scrapy-2.11.2 service-identity-24.1.0 setuptools-72.1.0 tldextract-5.1.2 typing-extensions-4.12.2 w3lib-2.2.1 zope.interface-7.0.1\n",
      "Collecting selenium\n",
      "  Downloading selenium-4.23.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in c:\\users\\mehta\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (2.2.2)\n",
      "Collecting trio~=0.17 (from selenium)\n",
      "  Downloading trio-0.26.2-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting trio-websocket~=0.9 (from selenium)\n",
      "  Downloading trio_websocket-0.11.1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\mehta\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from selenium) (2024.7.4)\n",
      "Requirement already satisfied: typing_extensions~=4.9 in c:\\users\\mehta\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from selenium) (4.12.2)\n",
      "Collecting websocket-client~=1.8 (from selenium)\n",
      "  Downloading websocket_client-1.8.0-py3-none-any.whl.metadata (8.0 kB)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\users\\mehta\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from trio~=0.17->selenium) (24.1.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\mehta\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\mehta\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from trio~=0.17->selenium) (3.7)\n",
      "Collecting outcome (from trio~=0.17->selenium)\n",
      "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting sniffio>=1.3.0 (from trio~=0.17->selenium)\n",
      "  Downloading sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\mehta\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from trio~=0.17->selenium) (1.17.0)\n",
      "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n",
      "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting pysocks!=1.5.7,<2.0,>=1.5.6 (from urllib3[socks]<3,>=1.26->selenium)\n",
      "  Downloading PySocks-1.7.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: pycparser in c:\\users\\mehta\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.22)\n",
      "Collecting h11<1,>=0.9.0 (from wsproto>=0.14->trio-websocket~=0.9->selenium)\n",
      "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Downloading selenium-4.23.1-py3-none-any.whl (9.4 MB)\n",
      "   ---------------------------------------- 0.0/9.4 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 1.0/9.4 MB 6.3 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 2.4/9.4 MB 6.7 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 3.9/9.4 MB 6.5 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 5.2/9.4 MB 6.8 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 6.8/9.4 MB 6.7 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 8.7/9.4 MB 6.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.4/9.4 MB 6.5 MB/s eta 0:00:00\n",
      "Downloading trio-0.26.2-py3-none-any.whl (475 kB)\n",
      "Downloading trio_websocket-0.11.1-py3-none-any.whl (17 kB)\n",
      "Downloading websocket_client-1.8.0-py3-none-any.whl (58 kB)\n",
      "Downloading PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
      "Downloading sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
      "Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
      "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Installing collected packages: websocket-client, sniffio, pysocks, outcome, h11, wsproto, trio, trio-websocket, selenium\n",
      "Successfully installed h11-0.14.0 outcome-1.3.0.post0 pysocks-1.7.1 selenium-4.23.1 sniffio-1.3.1 trio-0.26.2 trio-websocket-0.11.1 websocket-client-1.8.0 wsproto-1.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install beautifulsoup4\n",
    "!pip install scrapy\n",
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " we started with the first operation which was to get the data of the url and ids loaded which would be further use for the operations  as such this data then would be  used in a way to  get the textual data to be extracted then stored in the text file  this would be then used to  make use of scores which will be used  to get the result  and thus we can use this scores  to get a better insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(r'Input.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bctech2011</td>\n",
       "      <td>https://insights.blackcoffer.com/ml-and-ai-bas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bctech2012</td>\n",
       "      <td>https://insights.blackcoffer.com/streamlined-i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bctech2013</td>\n",
       "      <td>https://insights.blackcoffer.com/efficient-dat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bctech2014</td>\n",
       "      <td>https://insights.blackcoffer.com/effective-man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bctech2015</td>\n",
       "      <td>https://insights.blackcoffer.com/streamlined-t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       URL_ID                                                URL\n",
       "0  bctech2011  https://insights.blackcoffer.com/ml-and-ai-bas...\n",
       "1  bctech2012  https://insights.blackcoffer.com/streamlined-i...\n",
       "2  bctech2013  https://insights.blackcoffer.com/efficient-dat...\n",
       "3  bctech2014  https://insights.blackcoffer.com/effective-man...\n",
       "4  bctech2015  https://insights.blackcoffer.com/streamlined-t..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we begin with the intial task which is to extract words   from the  head  of title and the body of text  for the operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective:\n",
    "\n",
    "For each of the articles, given in the input.xlsx file, extract the article text and save the extracted article in a text file with URL_ID as its file name.\n",
    "While extracting text, please make sure your program extracts only the article title and the article text. It should not extract the website header, footer, or anything other than the article text. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_article_content(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  \n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        title = soup.find('h1', class_='entry-title').get_text(strip=True)\n",
    "        body_div = soup.find('div', class_='td-pb-span8 td-main-content')\n",
    "        body = body_div.get_text(separator='\\n', strip=True)\n",
    "        return title, body\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting content from {url}: {e}\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_article(url_id, title, body):\n",
    "    if not title or not body:\n",
    "        print(f\"Skipping URL_ID {url_id} due to missing content.\")\n",
    "        return\n",
    "    \n",
    "    file_name = f\"{url_id}.txt\"\n",
    "    with open(file_name, 'w', encoding='utf-8') as file:\n",
    "        file.write(title + '\\n\\n' + body)\n",
    "    print(f\"Article saved as {file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_articles(df):\n",
    "    for index, row in df.iterrows():\n",
    "        url_id = row['URL_ID']\n",
    "        url = row['URL']\n",
    "        print(f\"Processing URL_ID {url_id}...\")\n",
    "        title, body = extract_article_content(url)\n",
    "        save_article(url_id, title, body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing URL_ID bctech2011...\n",
      "Article saved as bctech2011.txt\n",
      "Processing URL_ID bctech2012...\n",
      "Article saved as bctech2012.txt\n",
      "Processing URL_ID bctech2013...\n",
      "Article saved as bctech2013.txt\n",
      "Processing URL_ID bctech2014...\n",
      "Article saved as bctech2014.txt\n",
      "Processing URL_ID bctech2015...\n",
      "Article saved as bctech2015.txt\n",
      "Processing URL_ID bctech2016...\n",
      "Article saved as bctech2016.txt\n",
      "Processing URL_ID bctech2017...\n",
      "Article saved as bctech2017.txt\n",
      "Processing URL_ID bctech2018...\n",
      "Article saved as bctech2018.txt\n",
      "Processing URL_ID bctech2019...\n",
      "Article saved as bctech2019.txt\n",
      "Processing URL_ID bctech2020...\n",
      "Article saved as bctech2020.txt\n",
      "Processing URL_ID bctech2021...\n",
      "Article saved as bctech2021.txt\n",
      "Processing URL_ID bctech2022...\n",
      "Article saved as bctech2022.txt\n",
      "Processing URL_ID bctech2023...\n",
      "Article saved as bctech2023.txt\n",
      "Processing URL_ID bctech2024...\n",
      "Article saved as bctech2024.txt\n",
      "Processing URL_ID bctech2025...\n",
      "Article saved as bctech2025.txt\n",
      "Processing URL_ID bctech2026...\n",
      "Article saved as bctech2026.txt\n",
      "Processing URL_ID bctech2027...\n",
      "Article saved as bctech2027.txt\n",
      "Processing URL_ID bctech2028...\n",
      "Article saved as bctech2028.txt\n",
      "Processing URL_ID bctech2029...\n",
      "Article saved as bctech2029.txt\n",
      "Processing URL_ID bctech2030...\n",
      "Article saved as bctech2030.txt\n",
      "Processing URL_ID bctech2031...\n",
      "Article saved as bctech2031.txt\n",
      "Processing URL_ID bctech2032...\n",
      "Article saved as bctech2032.txt\n",
      "Processing URL_ID bctech2033...\n",
      "Article saved as bctech2033.txt\n",
      "Processing URL_ID bctech2034...\n",
      "Article saved as bctech2034.txt\n",
      "Processing URL_ID bctech2035...\n",
      "Article saved as bctech2035.txt\n",
      "Processing URL_ID bctech2036...\n",
      "Article saved as bctech2036.txt\n",
      "Processing URL_ID bctech2037...\n",
      "Article saved as bctech2037.txt\n",
      "Processing URL_ID bctech2038...\n",
      "Article saved as bctech2038.txt\n",
      "Processing URL_ID bctech2039...\n",
      "Article saved as bctech2039.txt\n",
      "Processing URL_ID bctech2040...\n",
      "Article saved as bctech2040.txt\n",
      "Processing URL_ID bctech2041...\n",
      "Article saved as bctech2041.txt\n",
      "Processing URL_ID bctech2042...\n",
      "Article saved as bctech2042.txt\n",
      "Processing URL_ID bctech2043...\n",
      "Article saved as bctech2043.txt\n",
      "Processing URL_ID bctech2044...\n",
      "Article saved as bctech2044.txt\n",
      "Processing URL_ID bctech2045...\n",
      "Article saved as bctech2045.txt\n",
      "Processing URL_ID bctech2046...\n",
      "Article saved as bctech2046.txt\n",
      "Processing URL_ID bctech2047...\n",
      "Article saved as bctech2047.txt\n",
      "Processing URL_ID bctech2048...\n",
      "Article saved as bctech2048.txt\n",
      "Processing URL_ID bctech2049...\n",
      "Article saved as bctech2049.txt\n",
      "Processing URL_ID bctech2050...\n",
      "Article saved as bctech2050.txt\n",
      "Processing URL_ID bctech2051...\n",
      "Article saved as bctech2051.txt\n",
      "Processing URL_ID bctech2052...\n",
      "Article saved as bctech2052.txt\n",
      "Processing URL_ID bctech2053...\n",
      "Article saved as bctech2053.txt\n",
      "Processing URL_ID bctech2054...\n",
      "Article saved as bctech2054.txt\n",
      "Processing URL_ID bctech2055...\n",
      "Article saved as bctech2055.txt\n",
      "Processing URL_ID bctech2056...\n",
      "Article saved as bctech2056.txt\n",
      "Processing URL_ID bctech2057...\n",
      "Article saved as bctech2057.txt\n",
      "Processing URL_ID bctech2058...\n",
      "Article saved as bctech2058.txt\n",
      "Processing URL_ID bctech2059...\n",
      "Article saved as bctech2059.txt\n",
      "Processing URL_ID bctech2060...\n",
      "Article saved as bctech2060.txt\n",
      "Processing URL_ID bctech2061...\n",
      "Article saved as bctech2061.txt\n",
      "Processing URL_ID bctech2062...\n",
      "Article saved as bctech2062.txt\n",
      "Processing URL_ID bctech2063...\n",
      "Article saved as bctech2063.txt\n",
      "Processing URL_ID bctech2064...\n",
      "Article saved as bctech2064.txt\n",
      "Processing URL_ID bctech2065...\n",
      "Article saved as bctech2065.txt\n",
      "Processing URL_ID bctech2066...\n",
      "Article saved as bctech2066.txt\n",
      "Processing URL_ID bctech2067...\n",
      "Article saved as bctech2067.txt\n",
      "Processing URL_ID bctech2068...\n",
      "Article saved as bctech2068.txt\n",
      "Processing URL_ID bctech2069...\n",
      "Article saved as bctech2069.txt\n",
      "Processing URL_ID bctech2070...\n",
      "Article saved as bctech2070.txt\n",
      "Processing URL_ID bctech2071...\n",
      "Article saved as bctech2071.txt\n",
      "Processing URL_ID bctech2072...\n",
      "Article saved as bctech2072.txt\n",
      "Processing URL_ID bctech2073...\n",
      "Article saved as bctech2073.txt\n",
      "Processing URL_ID bctech2074...\n",
      "Article saved as bctech2074.txt\n",
      "Processing URL_ID bctech2075...\n",
      "Article saved as bctech2075.txt\n",
      "Processing URL_ID bctech2076...\n",
      "Article saved as bctech2076.txt\n",
      "Processing URL_ID bctech2077...\n",
      "Article saved as bctech2077.txt\n",
      "Processing URL_ID bctech2078...\n",
      "Article saved as bctech2078.txt\n",
      "Processing URL_ID bctech2079...\n",
      "Article saved as bctech2079.txt\n",
      "Processing URL_ID bctech2080...\n",
      "Article saved as bctech2080.txt\n",
      "Processing URL_ID bctech2081...\n",
      "Article saved as bctech2081.txt\n",
      "Processing URL_ID bctech2082...\n",
      "Article saved as bctech2082.txt\n",
      "Processing URL_ID bctech2083...\n",
      "Article saved as bctech2083.txt\n",
      "Processing URL_ID bctech2084...\n",
      "Article saved as bctech2084.txt\n",
      "Processing URL_ID bctech2085...\n",
      "Article saved as bctech2085.txt\n",
      "Processing URL_ID bctech2086...\n",
      "Article saved as bctech2086.txt\n",
      "Processing URL_ID bctech2087...\n",
      "Article saved as bctech2087.txt\n",
      "Processing URL_ID bctech2088...\n",
      "Article saved as bctech2088.txt\n",
      "Processing URL_ID bctech2089...\n",
      "Article saved as bctech2089.txt\n",
      "Processing URL_ID bctech2090...\n",
      "Article saved as bctech2090.txt\n",
      "Processing URL_ID bctech2091...\n",
      "Article saved as bctech2091.txt\n",
      "Processing URL_ID bctech2092...\n",
      "Article saved as bctech2092.txt\n",
      "Processing URL_ID bctech2093...\n",
      "Article saved as bctech2093.txt\n",
      "Processing URL_ID bctech2094...\n",
      "Article saved as bctech2094.txt\n",
      "Processing URL_ID bctech2095...\n",
      "Article saved as bctech2095.txt\n",
      "Processing URL_ID bctech2096...\n",
      "Article saved as bctech2096.txt\n",
      "Processing URL_ID bctech2097...\n",
      "Article saved as bctech2097.txt\n",
      "Processing URL_ID bctech2098...\n",
      "Article saved as bctech2098.txt\n",
      "Processing URL_ID bctech2099...\n",
      "Article saved as bctech2099.txt\n",
      "Processing URL_ID bctech2100...\n",
      "Article saved as bctech2100.txt\n",
      "Processing URL_ID bctech2101...\n",
      "Article saved as bctech2101.txt\n",
      "Processing URL_ID bctech2102...\n",
      "Article saved as bctech2102.txt\n",
      "Processing URL_ID bctech2103...\n",
      "Article saved as bctech2103.txt\n",
      "Processing URL_ID bctech2104...\n",
      "Article saved as bctech2104.txt\n",
      "Processing URL_ID bctech2105...\n",
      "Article saved as bctech2105.txt\n",
      "Processing URL_ID bctech2106...\n",
      "Article saved as bctech2106.txt\n",
      "Processing URL_ID bctech2107...\n",
      "Article saved as bctech2107.txt\n",
      "Processing URL_ID bctech2108...\n",
      "Article saved as bctech2108.txt\n",
      "Processing URL_ID bctech2109...\n",
      "Article saved as bctech2109.txt\n",
      "Processing URL_ID bctech2110...\n",
      "Article saved as bctech2110.txt\n",
      "Processing URL_ID bctech2111...\n",
      "Article saved as bctech2111.txt\n",
      "Processing URL_ID bctech2112...\n",
      "Article saved as bctech2112.txt\n",
      "Processing URL_ID bctech2113...\n",
      "Article saved as bctech2113.txt\n",
      "Processing URL_ID bctech2114...\n",
      "Article saved as bctech2114.txt\n",
      "Processing URL_ID bctech2115...\n",
      "Article saved as bctech2115.txt\n",
      "Processing URL_ID bctech2116...\n",
      "Article saved as bctech2116.txt\n",
      "Processing URL_ID bctech2117...\n",
      "Article saved as bctech2117.txt\n",
      "Processing URL_ID bctech2118...\n",
      "Article saved as bctech2118.txt\n",
      "Processing URL_ID bctech2119...\n",
      "Article saved as bctech2119.txt\n",
      "Processing URL_ID bctech2120...\n",
      "Article saved as bctech2120.txt\n",
      "Processing URL_ID bctech2121...\n",
      "Article saved as bctech2121.txt\n",
      "Processing URL_ID bctech2122...\n",
      "Article saved as bctech2122.txt\n",
      "Processing URL_ID bctech2123...\n",
      "Article saved as bctech2123.txt\n",
      "Processing URL_ID bctech2124...\n",
      "Article saved as bctech2124.txt\n",
      "Processing URL_ID bctech2125...\n",
      "Article saved as bctech2125.txt\n",
      "Processing URL_ID bctech2126...\n",
      "Article saved as bctech2126.txt\n",
      "Processing URL_ID bctech2127...\n",
      "Article saved as bctech2127.txt\n",
      "Processing URL_ID bctech2128...\n",
      "Article saved as bctech2128.txt\n",
      "Processing URL_ID bctech2129...\n",
      "Article saved as bctech2129.txt\n",
      "Processing URL_ID bctech2130...\n",
      "Article saved as bctech2130.txt\n",
      "Processing URL_ID bctech2131...\n",
      "Article saved as bctech2131.txt\n",
      "Processing URL_ID bctech2132...\n",
      "Article saved as bctech2132.txt\n",
      "Processing URL_ID bctech2133...\n",
      "Article saved as bctech2133.txt\n",
      "Processing URL_ID bctech2134...\n",
      "Article saved as bctech2134.txt\n",
      "Processing URL_ID bctech2135...\n",
      "Article saved as bctech2135.txt\n",
      "Processing URL_ID bctech2136...\n",
      "Article saved as bctech2136.txt\n",
      "Processing URL_ID bctech2137...\n",
      "Article saved as bctech2137.txt\n",
      "Processing URL_ID bctech2138...\n",
      "Article saved as bctech2138.txt\n",
      "Processing URL_ID bctech2139...\n",
      "Article saved as bctech2139.txt\n",
      "Processing URL_ID bctech2140...\n",
      "Article saved as bctech2140.txt\n",
      "Processing URL_ID bctech2141...\n",
      "Article saved as bctech2141.txt\n",
      "Processing URL_ID bctech2142...\n",
      "Article saved as bctech2142.txt\n",
      "Processing URL_ID bctech2143...\n",
      "Article saved as bctech2143.txt\n",
      "Processing URL_ID bctech2144...\n",
      "Article saved as bctech2144.txt\n",
      "Processing URL_ID bctech2145...\n",
      "Article saved as bctech2145.txt\n",
      "Processing URL_ID bctech2146...\n",
      "Article saved as bctech2146.txt\n",
      "Processing URL_ID bctech2147...\n",
      "Article saved as bctech2147.txt\n",
      "Processing URL_ID bctech2148...\n",
      "Article saved as bctech2148.txt\n",
      "Processing URL_ID bctech2149...\n",
      "Article saved as bctech2149.txt\n",
      "Processing URL_ID bctech2150...\n",
      "Article saved as bctech2150.txt\n",
      "Processing URL_ID bctech2151...\n",
      "Article saved as bctech2151.txt\n",
      "Processing URL_ID bctech2152...\n",
      "Article saved as bctech2152.txt\n",
      "Processing URL_ID bctech2153...\n",
      "Article saved as bctech2153.txt\n",
      "Processing URL_ID bctech2154...\n",
      "Article saved as bctech2154.txt\n",
      "Processing URL_ID bctech2155...\n",
      "Article saved as bctech2155.txt\n",
      "Processing URL_ID bctech2156...\n",
      "Article saved as bctech2156.txt\n",
      "Processing URL_ID bctech2157...\n",
      "Article saved as bctech2157.txt\n"
     ]
    }
   ],
   "source": [
    "process_articles(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the  next task begins  which is to   the first with the  removal of the stop words from the given  extracted text  so that the processed text files then can be used for getting the insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.8.2-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting click (from nltk)\n",
      "  Using cached click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting joblib (from nltk)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2024.7.24-cp312-cp312-win_amd64.whl.metadata (41 kB)\n",
      "Collecting tqdm (from nltk)\n",
      "  Downloading tqdm-4.66.5-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\mehta\\appdata\\roaming\\python\\python312\\site-packages (from click->nltk) (0.4.6)\n",
      "Downloading nltk-3.8.2-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 0.3/1.5 MB ? eta -:--:--\n",
      "   -------------------- ------------------- 0.8/1.5 MB 2.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 3.1 MB/s eta 0:00:00\n",
      "Downloading regex-2024.7.24-cp312-cp312-win_amd64.whl (269 kB)\n",
      "Using cached click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Downloading tqdm-4.66.5-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, regex, joblib, click, nltk\n",
      "Successfully installed click-8.1.7 joblib-1.4.2 nltk-3.8.2 regex-2024.7.24 tqdm-4.66.5\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_stop_words(stopwords_folder):\n",
    "    stop_words = []\n",
    "    print(\"Loading stop words from the provided folder...\")\n",
    "    for filename in os.listdir(stopwords_folder):\n",
    "        filepath = os.path.join(stopwords_folder, filename)\n",
    "        try:\n",
    "            with open(filepath, 'r', encoding='utf-8', errors='replace') as file:\n",
    "                for line in file:\n",
    "                    word = line.strip().lower()\n",
    "                    if word and word not in stop_words:\n",
    "                        stop_words.append(word)\n",
    "        except UnicodeDecodeError:\n",
    "            print(f\"UnicodeDecodeError encountered in file {filename}. Trying with ISO-8859-1 encoding.\")\n",
    "            with open(filepath, 'r', encoding='ISO-8859-1', errors='replace') as file:\n",
    "                for line in file:\n",
    "                    word = line.strip().lower()\n",
    "                    if word and word not in stop_words:  \n",
    "                        stop_words.append(word)\n",
    "                    \n",
    "    print(f\"Loaded {len(stop_words)} stop words.\")\n",
    "    return stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text, stop_words):\n",
    "    print(\"Cleaning text by removing stop words...\")\n",
    "    words = text.split()\n",
    "    cleaned_words = [word for word in words if word.lower() not in stop_words]\n",
    "    cleaned_text = ' '.join(cleaned_words)\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_articles(directory, stop_words, df):\n",
    "    # Automatically set the processed directory path\n",
    "    processed_directory = os.path.join(directory, 'processed_files')\n",
    "    \n",
    "    print(\"Starting the processing of articles...\")\n",
    "    \n",
    "    # Create the processed directory if it doesn't exist\n",
    "    if not os.path.exists(processed_directory):\n",
    "        os.makedirs(processed_directory)\n",
    "        print(f\"Created directory: {processed_directory}\")\n",
    "    \n",
    "    # Iterate through the files listed in the DataFrame\n",
    "    for url_id in df['URL_ID']:\n",
    "        filename = f\"{url_id}.txt\"\n",
    "        \n",
    "        if filename in os.listdir(directory):\n",
    "            print(f\"Processing file: {filename}...\")\n",
    "            \n",
    "            filepath = os.path.join(directory, filename)\n",
    "            with open(filepath, 'r', encoding='utf-8') as file:\n",
    "                text = file.read()\n",
    "            \n",
    "            # Clean the text by removing stop words\n",
    "            cleaned_text = clean_text(text, stop_words)\n",
    "            \n",
    "            # Save the cleaned text to the processed directory\n",
    "            processed_filepath = os.path.join(processed_directory, filename)\n",
    "            with open(processed_filepath, 'w', encoding='utf-8') as file:\n",
    "                file.write(cleaned_text)\n",
    "                \n",
    "            print(f\"Finished processing {filename}. Saved to {processed_filepath}.\")\n",
    "    \n",
    "    print(\"All specified articles have been processed and saved to the processed directory.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading stop words from the provided folder...\n",
      "Loaded 12768 stop words.\n",
      "Starting the processing of articles...\n",
      "Created directory: C:/20211030 Test Assignment\\processed_files\n",
      "Processing file: bctech2011.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2011.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2011.txt.\n",
      "Processing file: bctech2012.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2012.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2012.txt.\n",
      "Processing file: bctech2013.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2013.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2013.txt.\n",
      "Processing file: bctech2014.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2014.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2014.txt.\n",
      "Processing file: bctech2015.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2015.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2015.txt.\n",
      "Processing file: bctech2016.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2016.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2016.txt.\n",
      "Processing file: bctech2017.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2017.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2017.txt.\n",
      "Processing file: bctech2018.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2018.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2018.txt.\n",
      "Processing file: bctech2019.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2019.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2019.txt.\n",
      "Processing file: bctech2020.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2020.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2020.txt.\n",
      "Processing file: bctech2021.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2021.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2021.txt.\n",
      "Processing file: bctech2022.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2022.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2022.txt.\n",
      "Processing file: bctech2023.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2023.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2023.txt.\n",
      "Processing file: bctech2024.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2024.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2024.txt.\n",
      "Processing file: bctech2025.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2025.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2025.txt.\n",
      "Processing file: bctech2026.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2026.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2026.txt.\n",
      "Processing file: bctech2027.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2027.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2027.txt.\n",
      "Processing file: bctech2028.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2028.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2028.txt.\n",
      "Processing file: bctech2029.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2029.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2029.txt.\n",
      "Processing file: bctech2030.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2030.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2030.txt.\n",
      "Processing file: bctech2031.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2031.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2031.txt.\n",
      "Processing file: bctech2032.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2032.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2032.txt.\n",
      "Processing file: bctech2033.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2033.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2033.txt.\n",
      "Processing file: bctech2034.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2034.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2034.txt.\n",
      "Processing file: bctech2035.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2035.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2035.txt.\n",
      "Processing file: bctech2036.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2036.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2036.txt.\n",
      "Processing file: bctech2037.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2037.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2037.txt.\n",
      "Processing file: bctech2038.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2038.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2038.txt.\n",
      "Processing file: bctech2039.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2039.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2039.txt.\n",
      "Processing file: bctech2040.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2040.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2040.txt.\n",
      "Processing file: bctech2041.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2041.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2041.txt.\n",
      "Processing file: bctech2042.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2042.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2042.txt.\n",
      "Processing file: bctech2043.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2043.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2043.txt.\n",
      "Processing file: bctech2044.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2044.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2044.txt.\n",
      "Processing file: bctech2045.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2045.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2045.txt.\n",
      "Processing file: bctech2046.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2046.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2046.txt.\n",
      "Processing file: bctech2047.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2047.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2047.txt.\n",
      "Processing file: bctech2048.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2048.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2048.txt.\n",
      "Processing file: bctech2049.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2049.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2049.txt.\n",
      "Processing file: bctech2050.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2050.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2050.txt.\n",
      "Processing file: bctech2051.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2051.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2051.txt.\n",
      "Processing file: bctech2052.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2052.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2052.txt.\n",
      "Processing file: bctech2053.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2053.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2053.txt.\n",
      "Processing file: bctech2054.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2054.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2054.txt.\n",
      "Processing file: bctech2055.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2055.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2055.txt.\n",
      "Processing file: bctech2056.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2056.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2056.txt.\n",
      "Processing file: bctech2057.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2057.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2057.txt.\n",
      "Processing file: bctech2058.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2058.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2058.txt.\n",
      "Processing file: bctech2059.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2059.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2059.txt.\n",
      "Processing file: bctech2060.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2060.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2060.txt.\n",
      "Processing file: bctech2061.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2061.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2061.txt.\n",
      "Processing file: bctech2062.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2062.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2062.txt.\n",
      "Processing file: bctech2063.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2063.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2063.txt.\n",
      "Processing file: bctech2064.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2064.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2064.txt.\n",
      "Processing file: bctech2065.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2065.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2065.txt.\n",
      "Processing file: bctech2066.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2066.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2066.txt.\n",
      "Processing file: bctech2067.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2067.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2067.txt.\n",
      "Processing file: bctech2068.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2068.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2068.txt.\n",
      "Processing file: bctech2069.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2069.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2069.txt.\n",
      "Processing file: bctech2070.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2070.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2070.txt.\n",
      "Processing file: bctech2071.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2071.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2071.txt.\n",
      "Processing file: bctech2072.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2072.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2072.txt.\n",
      "Processing file: bctech2073.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2073.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2073.txt.\n",
      "Processing file: bctech2074.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2074.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2074.txt.\n",
      "Processing file: bctech2075.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2075.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2075.txt.\n",
      "Processing file: bctech2076.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2076.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2076.txt.\n",
      "Processing file: bctech2077.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2077.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2077.txt.\n",
      "Processing file: bctech2078.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2078.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2078.txt.\n",
      "Processing file: bctech2079.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2079.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2079.txt.\n",
      "Processing file: bctech2080.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2080.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2080.txt.\n",
      "Processing file: bctech2081.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2081.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2081.txt.\n",
      "Processing file: bctech2082.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2082.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2082.txt.\n",
      "Processing file: bctech2083.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2083.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2083.txt.\n",
      "Processing file: bctech2084.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2084.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2084.txt.\n",
      "Processing file: bctech2085.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2085.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2085.txt.\n",
      "Processing file: bctech2086.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2086.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2086.txt.\n",
      "Processing file: bctech2087.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2087.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2087.txt.\n",
      "Processing file: bctech2088.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2088.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2088.txt.\n",
      "Processing file: bctech2089.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2089.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2089.txt.\n",
      "Processing file: bctech2090.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2090.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2090.txt.\n",
      "Processing file: bctech2091.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2091.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2091.txt.\n",
      "Processing file: bctech2092.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2092.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2092.txt.\n",
      "Processing file: bctech2093.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2093.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2093.txt.\n",
      "Processing file: bctech2094.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2094.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2094.txt.\n",
      "Processing file: bctech2095.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2095.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2095.txt.\n",
      "Processing file: bctech2096.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2096.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2096.txt.\n",
      "Processing file: bctech2097.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2097.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2097.txt.\n",
      "Processing file: bctech2098.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2098.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2098.txt.\n",
      "Processing file: bctech2099.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2099.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2099.txt.\n",
      "Processing file: bctech2100.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2100.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2100.txt.\n",
      "Processing file: bctech2101.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2101.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2101.txt.\n",
      "Processing file: bctech2102.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2102.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2102.txt.\n",
      "Processing file: bctech2103.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2103.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2103.txt.\n",
      "Processing file: bctech2104.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2104.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2104.txt.\n",
      "Processing file: bctech2105.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2105.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2105.txt.\n",
      "Processing file: bctech2106.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2106.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2106.txt.\n",
      "Processing file: bctech2107.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2107.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2107.txt.\n",
      "Processing file: bctech2108.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2108.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2108.txt.\n",
      "Processing file: bctech2109.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2109.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2109.txt.\n",
      "Processing file: bctech2110.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2110.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2110.txt.\n",
      "Processing file: bctech2111.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2111.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2111.txt.\n",
      "Processing file: bctech2112.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2112.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2112.txt.\n",
      "Processing file: bctech2113.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2113.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2113.txt.\n",
      "Processing file: bctech2114.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2114.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2114.txt.\n",
      "Processing file: bctech2115.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2115.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2115.txt.\n",
      "Processing file: bctech2116.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2116.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2116.txt.\n",
      "Processing file: bctech2117.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2117.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2117.txt.\n",
      "Processing file: bctech2118.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2118.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2118.txt.\n",
      "Processing file: bctech2119.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2119.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2119.txt.\n",
      "Processing file: bctech2120.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2120.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2120.txt.\n",
      "Processing file: bctech2121.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2121.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2121.txt.\n",
      "Processing file: bctech2122.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2122.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2122.txt.\n",
      "Processing file: bctech2123.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2123.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2123.txt.\n",
      "Processing file: bctech2124.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2124.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2124.txt.\n",
      "Processing file: bctech2125.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2125.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2125.txt.\n",
      "Processing file: bctech2126.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2126.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2126.txt.\n",
      "Processing file: bctech2127.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2127.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2127.txt.\n",
      "Processing file: bctech2128.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2128.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2128.txt.\n",
      "Processing file: bctech2129.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2129.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2129.txt.\n",
      "Processing file: bctech2130.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2130.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2130.txt.\n",
      "Processing file: bctech2131.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2131.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2131.txt.\n",
      "Processing file: bctech2132.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2132.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2132.txt.\n",
      "Processing file: bctech2133.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2133.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2133.txt.\n",
      "Processing file: bctech2134.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2134.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2134.txt.\n",
      "Processing file: bctech2135.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2135.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2135.txt.\n",
      "Processing file: bctech2136.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2136.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2136.txt.\n",
      "Processing file: bctech2137.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2137.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2137.txt.\n",
      "Processing file: bctech2138.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2138.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2138.txt.\n",
      "Processing file: bctech2139.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2139.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2139.txt.\n",
      "Processing file: bctech2140.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2140.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2140.txt.\n",
      "Processing file: bctech2141.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2141.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2141.txt.\n",
      "Processing file: bctech2142.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2142.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2142.txt.\n",
      "Processing file: bctech2143.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2143.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2143.txt.\n",
      "Processing file: bctech2144.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2144.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2144.txt.\n",
      "Processing file: bctech2145.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2145.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2145.txt.\n",
      "Processing file: bctech2146.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2146.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2146.txt.\n",
      "Processing file: bctech2147.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2147.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2147.txt.\n",
      "Processing file: bctech2148.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2148.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2148.txt.\n",
      "Processing file: bctech2149.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2149.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2149.txt.\n",
      "Processing file: bctech2150.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2150.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2150.txt.\n",
      "Processing file: bctech2151.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2151.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2151.txt.\n",
      "Processing file: bctech2152.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2152.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2152.txt.\n",
      "Processing file: bctech2153.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2153.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2153.txt.\n",
      "Processing file: bctech2154.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2154.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2154.txt.\n",
      "Processing file: bctech2155.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2155.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2155.txt.\n",
      "Processing file: bctech2156.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2156.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2156.txt.\n",
      "Processing file: bctech2157.txt...\n",
      "Cleaning text by removing stop words...\n",
      "Finished processing bctech2157.txt. Saved to C:/20211030 Test Assignment\\processed_files\\bctech2157.txt.\n",
      "All specified articles have been processed and saved to the processed directory.\n"
     ]
    }
   ],
   "source": [
    "stopwords_folder = 'C:/20211030 Test Assignment/StopWords'\n",
    "articles_directory = 'C:/20211030 Test Assignment'\n",
    "stop_words = load_stop_words(stopwords_folder)\n",
    "process_articles(articles_directory, stop_words, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "counting positive and negative words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_word_dict(file_path):\n",
    "    \n",
    "    words_dict = {}\n",
    "    with open(file_path, 'r') as file:\n",
    "        for word in file.read().strip().split():\n",
    "            words_dict[word] = 0\n",
    "    return words_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_scores(text, positive_dict, negative_dict):\n",
    "    \"\"\"Calculates positive, negative scores, and total words for a given text.\"\"\"\n",
    "    nltk.download('punkt')  # Ensure the 'punkt' tokenizer is downloaded\n",
    "    tokens = word_tokenize(text.lower())  # Tokenize and convert to lowercase\n",
    "    positive_score = sum(1 for token in tokens if token in positive_dict)\n",
    "    negative_score = -sum(1 for token in tokens if token in negative_dict)\n",
    "    total_words = len(tokens)  # Calculate total words\n",
    "    \n",
    "    return positive_score, abs(negative_score), total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_dataframe(df, positive_dict, negative_dict, processed_files_dir):\n",
    "    pos_scores = []\n",
    "    neg_scores = []\n",
    "    polarity_scores = []\n",
    "    subjectivity_scores = []\n",
    "    for url_id in df['URL_ID']:\n",
    "        filename = f\"{url_id}.txt\"\n",
    "        file_path = os.path.join(processed_files_dir, filename)\n",
    "        if os.path.exists(file_path):\n",
    "            with open(file_path, 'r',encoding='UTF-8') as file:\n",
    "                text = file.read()\n",
    "            pos_score, neg_score, total_words = calculate_scores(text, positive_dict, negative_dict)\n",
    "            pos_scores.append(pos_score)\n",
    "            neg_scores.append(neg_score)\n",
    "            polarity_score = (pos_score - neg_score) / ((pos_score + neg_score) + 0.000001)\n",
    "            polarity_scores.append(polarity_score)\n",
    "            subjectivity_score = (pos_score + neg_score) / (total_words + 0.000001)\n",
    "            subjectivity_scores.append(subjectivity_score)\n",
    "        else:\n",
    "            pos_scores.append(0)\n",
    "            neg_scores.append(0)\n",
    "            polarity_scores.append(0)\n",
    "            subjectivity_scores.append(0)\n",
    "    df['POSITIVE SCORE'] = pos_scores\n",
    "    df['NEGATIVE SCORE'] = neg_scores\n",
    "    df['POLARITY SCORE'] = polarity_scores\n",
    "    df['SUBJECTIVITY SCORE'] = subjectivity_scores\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "master_dict_dir = 'C:/20211030 Test Assignment/MasterDictionary'\n",
    "positive_file = os.path.join(master_dict_dir, 'positive-words.txt')\n",
    "negative_file = os.path.join(master_dict_dir, 'negative-words.txt')\n",
    "processed_files_dir = 'C:/20211030 Test Assignment/processed_files'\n",
    "\n",
    "\n",
    "positive_dict = create_word_dict(positive_file)\n",
    "negative_dict = create_word_dict(negative_file)\n",
    "\n",
    "updated_df = update_dataframe(df, positive_dict, negative_dict, processed_files_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       URL_ID                                                URL  \\\n",
      "0  bctech2011  https://insights.blackcoffer.com/ml-and-ai-bas...   \n",
      "1  bctech2012  https://insights.blackcoffer.com/streamlined-i...   \n",
      "2  bctech2013  https://insights.blackcoffer.com/efficient-dat...   \n",
      "3  bctech2014  https://insights.blackcoffer.com/effective-man...   \n",
      "4  bctech2015  https://insights.blackcoffer.com/streamlined-t...   \n",
      "\n",
      "   POSITIVE SCORE  NEGATIVE SCORE  POLARITY SCORE  SUBJECTIVITY SCORE  \n",
      "0             134              44        0.505618            0.076790  \n",
      "1              25               5        0.666667            0.060362  \n",
      "2              24               9        0.454545            0.054636  \n",
      "3              16               5        0.523809            0.042857  \n",
      "4              18               1        0.894737            0.034991  \n"
     ]
    }
   ],
   "source": [
    "print(updated_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       POSITIVE SCORE  NEGATIVE SCORE  POLARITY SCORE  SUBJECTIVITY SCORE\n",
      "count      147.000000      147.000000      147.000000          147.000000\n",
      "mean        12.639456        4.884354        0.466686            0.043590\n",
      "std         12.905317        5.465354        0.343512            0.018725\n",
      "min          2.000000        0.000000       -0.500000            0.006289\n",
      "25%          6.000000        1.500000        0.233032            0.031100\n",
      "50%         10.000000        4.000000        0.500000            0.040000\n",
      "75%         15.000000        6.000000        0.714286            0.054367\n",
      "max        134.000000       44.000000        1.000000            0.093496\n"
     ]
    }
   ],
   "source": [
    "print(updated_df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package cmudict to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package cmudict is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('cmudict')\n",
    "from nltk.tokenize import  sent_tokenize\n",
    "from nltk.corpus import cmudict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = cmudict.dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_syllables(word):\n",
    "    word = word.lower()\n",
    "    if word in d:\n",
    "        return max([len(list(y for y in x if y[-1].isdigit())) for x in d[word]])\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sentence_length(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    num_sentences = len(sentences)\n",
    "    tokens = word_tokenize(text)\n",
    "    num_words = len(tokens)\n",
    "    if num_sentences > 0:\n",
    "        avg_sentence_length = num_words / num_sentences\n",
    "    else:\n",
    "        avg_sentence_length = 0\n",
    "    return avg_sentence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_percentage_complex_words(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    num_words = len(tokens)\n",
    "    complex_words = sum(1 for token in tokens if count_syllables(token) > 2)\n",
    "    if num_words > 0:\n",
    "        percentage_complex_words = complex_words / num_words\n",
    "    else:\n",
    "        percentage_complex_words = 0\n",
    "    return percentage_complex_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_fog_index(text):\n",
    "    avg_sentence_length = calculate_sentence_length(text)\n",
    "    percentage_complex_words = calculate_percentage_complex_words(text)\n",
    "    fog_index = 0.4 * (avg_sentence_length + percentage_complex_words)\n",
    "    return fog_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Analysis_of_Readability(df, processed_files_dir):\n",
    "    avg_sentence_lengths = []\n",
    "    percentages_complex_words = []\n",
    "    fog_indexes = []\n",
    "    for url_id in df['URL_ID']:\n",
    "        filename = f\"{url_id}.txt\"\n",
    "        file_path = os.path.join(processed_files_dir, filename)\n",
    "        if os.path.exists(file_path):\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                text = file.read()\n",
    "            avg_sentence_length = calculate_sentence_length(text)\n",
    "            percentage_complex_words = calculate_percentage_complex_words(text)\n",
    "            fog_index = calculate_fog_index(text)\n",
    "            avg_sentence_lengths.append(avg_sentence_length)\n",
    "            percentages_complex_words.append(percentage_complex_words)\n",
    "            fog_indexes.append(fog_index)\n",
    "        else:\n",
    "            avg_sentence_lengths.append(0)\n",
    "            percentages_complex_words.append(0)\n",
    "            fog_indexes.append(0)\n",
    "    df['AVG SENTENCE LENGTH'] = avg_sentence_lengths\n",
    "    df['PERCENTAGE COMPLEX WORDS'] = percentages_complex_words\n",
    "    df['FOG INDEX'] = fog_indexes\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_files_dir = 'C:/20211030 Test Assignment/processed_files'\n",
    "updated_df = Analysis_of_Readability(updated_df,  processed_files_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       URL_ID                                                URL  \\\n",
      "0  bctech2011  https://insights.blackcoffer.com/ml-and-ai-bas...   \n",
      "1  bctech2012  https://insights.blackcoffer.com/streamlined-i...   \n",
      "2  bctech2013  https://insights.blackcoffer.com/efficient-dat...   \n",
      "3  bctech2014  https://insights.blackcoffer.com/effective-man...   \n",
      "4  bctech2015  https://insights.blackcoffer.com/streamlined-t...   \n",
      "\n",
      "   POSITIVE SCORE  NEGATIVE SCORE  POLARITY SCORE  SUBJECTIVITY SCORE  \\\n",
      "0             134              44        0.505618            0.076790   \n",
      "1              25               5        0.666667            0.060362   \n",
      "2              24               9        0.454545            0.054636   \n",
      "3              16               5        0.523809            0.042857   \n",
      "4              18               1        0.894737            0.034991   \n",
      "\n",
      "   AVG SENTENCE LENGTH  PERCENTAGE COMPLEX WORDS  FOG INDEX  \n",
      "0            13.237288                  0.373598   5.444354  \n",
      "1             9.557692                  0.340040   3.959093  \n",
      "2            17.257143                  0.274834   7.012791  \n",
      "3             9.245283                  0.336735   3.832807  \n",
      "4            18.724138                  0.305709   7.611939  \n"
     ]
    }
   ],
   "source": [
    "print(updated_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       POSITIVE SCORE  NEGATIVE SCORE  POLARITY SCORE  SUBJECTIVITY SCORE  \\\n",
      "count      147.000000      147.000000      147.000000          147.000000   \n",
      "mean        12.639456        4.884354        0.466686            0.043590   \n",
      "std         12.905317        5.465354        0.343512            0.018725   \n",
      "min          2.000000        0.000000       -0.500000            0.006289   \n",
      "25%          6.000000        1.500000        0.233032            0.031100   \n",
      "50%         10.000000        4.000000        0.500000            0.040000   \n",
      "75%         15.000000        6.000000        0.714286            0.054367   \n",
      "max        134.000000       44.000000        1.000000            0.093496   \n",
      "\n",
      "       AVG SENTENCE LENGTH  PERCENTAGE COMPLEX WORDS   FOG INDEX  \n",
      "count           147.000000                147.000000  147.000000  \n",
      "mean             24.517210                  0.248681    9.906356  \n",
      "std              20.494290                  0.047848    8.197381  \n",
      "min               8.811321                  0.140845    3.659860  \n",
      "25%              16.645833                  0.218723    6.747206  \n",
      "50%              18.906977                  0.245318    7.653687  \n",
      "75%              24.131373                  0.276203    9.739329  \n",
      "max             178.000000                  0.400000   71.314607  \n"
     ]
    }
   ],
   "source": [
    "print(updated_df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_avg_words_per_sentence(text):\n",
    "    \"\"\"Calculates the average number of words per sentence.\"\"\"\n",
    "    sentences = sent_tokenize(text)\n",
    "    total_sentences = len(sentences)\n",
    "    total_words = sum(len(word_tokenize(sentence)) for sentence in sentences)\n",
    "    \n",
    "    avg_words_per_sentence = total_words / total_sentences if total_sentences > 0 else 0\n",
    "    return avg_words_per_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_complex_words(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    complex_word_count = sum(1 for token in tokens if count_syllables(token) > 2)\n",
    "    return complex_word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_sen_and_complex_word_count(df, processed_files_dir):\n",
    "    avg_sentence_lengths = []\n",
    "    complex_word_counts = []\n",
    "    for url_id in df['URL_ID']:\n",
    "        filename = f\"{url_id}.txt\"\n",
    "        file_path = os.path.join(processed_files_dir, filename)\n",
    "        if os.path.exists(file_path):\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                text = file.read()\n",
    "            avg_sentence_length = calculate_avg_words_per_sentence(text)\n",
    "            complex_word_count = count_complex_words(text)\n",
    "            avg_sentence_lengths.append(avg_sentence_length)\n",
    "            complex_word_counts.append(complex_word_count)\n",
    "        \n",
    "        else:\n",
    "            avg_sentence_lengths.append(0)\n",
    "            complex_word_counts.append(0)\n",
    "    df['AVG WORDS PER SENTENCE LENGTH'] = avg_sentence_lengths\n",
    "    df['COMPLEX WORD COUNT'] = complex_word_counts\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_df =  avg_sen_and_complex_word_count(updated_df,processed_files_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "      <th>POSITIVE SCORE</th>\n",
       "      <th>NEGATIVE SCORE</th>\n",
       "      <th>POLARITY SCORE</th>\n",
       "      <th>SUBJECTIVITY SCORE</th>\n",
       "      <th>AVG SENTENCE LENGTH</th>\n",
       "      <th>PERCENTAGE COMPLEX WORDS</th>\n",
       "      <th>FOG INDEX</th>\n",
       "      <th>AVG WORDS PER SENTENCE LENGTH</th>\n",
       "      <th>COMPLEX WORD COUNT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bctech2011</td>\n",
       "      <td>https://insights.blackcoffer.com/ml-and-ai-bas...</td>\n",
       "      <td>134</td>\n",
       "      <td>44</td>\n",
       "      <td>0.505618</td>\n",
       "      <td>0.076790</td>\n",
       "      <td>13.237288</td>\n",
       "      <td>0.373598</td>\n",
       "      <td>5.444354</td>\n",
       "      <td>13.237288</td>\n",
       "      <td>866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bctech2012</td>\n",
       "      <td>https://insights.blackcoffer.com/streamlined-i...</td>\n",
       "      <td>25</td>\n",
       "      <td>5</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.060362</td>\n",
       "      <td>9.557692</td>\n",
       "      <td>0.340040</td>\n",
       "      <td>3.959093</td>\n",
       "      <td>9.557692</td>\n",
       "      <td>169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bctech2013</td>\n",
       "      <td>https://insights.blackcoffer.com/efficient-dat...</td>\n",
       "      <td>24</td>\n",
       "      <td>9</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.054636</td>\n",
       "      <td>17.257143</td>\n",
       "      <td>0.274834</td>\n",
       "      <td>7.012791</td>\n",
       "      <td>17.257143</td>\n",
       "      <td>166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bctech2014</td>\n",
       "      <td>https://insights.blackcoffer.com/effective-man...</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>0.523809</td>\n",
       "      <td>0.042857</td>\n",
       "      <td>9.245283</td>\n",
       "      <td>0.336735</td>\n",
       "      <td>3.832807</td>\n",
       "      <td>9.245283</td>\n",
       "      <td>165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bctech2015</td>\n",
       "      <td>https://insights.blackcoffer.com/streamlined-t...</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>0.894737</td>\n",
       "      <td>0.034991</td>\n",
       "      <td>18.724138</td>\n",
       "      <td>0.305709</td>\n",
       "      <td>7.611939</td>\n",
       "      <td>18.724138</td>\n",
       "      <td>166</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       URL_ID                                                URL  \\\n",
       "0  bctech2011  https://insights.blackcoffer.com/ml-and-ai-bas...   \n",
       "1  bctech2012  https://insights.blackcoffer.com/streamlined-i...   \n",
       "2  bctech2013  https://insights.blackcoffer.com/efficient-dat...   \n",
       "3  bctech2014  https://insights.blackcoffer.com/effective-man...   \n",
       "4  bctech2015  https://insights.blackcoffer.com/streamlined-t...   \n",
       "\n",
       "   POSITIVE SCORE  NEGATIVE SCORE  POLARITY SCORE  SUBJECTIVITY SCORE  \\\n",
       "0             134              44        0.505618            0.076790   \n",
       "1              25               5        0.666667            0.060362   \n",
       "2              24               9        0.454545            0.054636   \n",
       "3              16               5        0.523809            0.042857   \n",
       "4              18               1        0.894737            0.034991   \n",
       "\n",
       "   AVG SENTENCE LENGTH  PERCENTAGE COMPLEX WORDS  FOG INDEX  \\\n",
       "0            13.237288                  0.373598   5.444354   \n",
       "1             9.557692                  0.340040   3.959093   \n",
       "2            17.257143                  0.274834   7.012791   \n",
       "3             9.245283                  0.336735   3.832807   \n",
       "4            18.724138                  0.305709   7.611939   \n",
       "\n",
       "   AVG WORDS PER SENTENCE LENGTH  COMPLEX WORD COUNT  \n",
       "0                      13.237288                 866  \n",
       "1                       9.557692                 169  \n",
       "2                      17.257143                 166  \n",
       "3                       9.245283                 165  \n",
       "4                      18.724138                 166  "
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import cmudict\n",
    "import string\n",
    "import nltk\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package cmudict to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package cmudict is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('cmudict')\n",
    "\n",
    "d = cmudict.dict()\n",
    "stop_words = set(stopwords.words('english'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_word_count(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    cleaned_tokens = [word.strip(string.punctuation) for word in tokens if word.lower() not in stop_words and word.isalpha()]\n",
    "    return len(cleaned_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_syllable_count_per_word(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    syllable_counts = [count_syllables(word) for word in tokens if word.isalpha()]\n",
    "    return syllable_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_personal_pronouns(text):\n",
    "    pronouns = re.findall(r'\\b(I|we|my|ours|us|me|mine|our|yours|your|they|them|theirs)\\b', text, re.IGNORECASE)\n",
    "    return len(pronouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_avg_word_length(text):\n",
    "    words = word_tokenize(text.lower())\n",
    "    words = [word for word in words if word.isalpha()]  \n",
    "    total_chars = sum(len(word) for word in words)\n",
    "    return total_chars / len(words) if words else 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_dataframe(df, processed_files_dir):\n",
    "    word_counts = []\n",
    "    syllable_counts = []\n",
    "    personal_pronoun_counts = []\n",
    "    avg_word_lengths = []\n",
    "    for url_id in df['URL_ID']:\n",
    "        filename = f\"{url_id}.txt\"\n",
    "        file_path = os.path.join(processed_files_dir, filename)\n",
    "        if os.path.exists(file_path):\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                text = file.read()\n",
    "            word_count = calculate_word_count(text)\n",
    "            syllable_count = sum(count_syllables(word) for word in word_tokenize(text.lower()) if word.isalpha())\n",
    "            personal_pronoun_count = count_personal_pronouns(text)\n",
    "            avg_word_length = calculate_avg_word_length(text)\n",
    "            word_counts.append(word_count)\n",
    "            syllable_counts.append(syllable_count)\n",
    "            personal_pronoun_counts.append(personal_pronoun_count)\n",
    "            avg_word_lengths.append(avg_word_length)\n",
    "        else:\n",
    "            word_counts.append(0)\n",
    "            syllable_counts.append(0)\n",
    "            personal_pronoun_counts.append(0)\n",
    "            avg_word_lengths.append(0)\n",
    "    df['WORD COUNT'] = word_counts\n",
    "    df['SYLLABLE COUNT'] = syllable_counts\n",
    "    df['PERSONAL PRONOUN COUNT'] = personal_pronoun_counts\n",
    "    df['AVG WORD LENGTH'] = avg_word_lengths\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_df = update_dataframe(updated_df,processed_files_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "      <th>POSITIVE SCORE</th>\n",
       "      <th>NEGATIVE SCORE</th>\n",
       "      <th>POLARITY SCORE</th>\n",
       "      <th>SUBJECTIVITY SCORE</th>\n",
       "      <th>AVG SENTENCE LENGTH</th>\n",
       "      <th>PERCENTAGE COMPLEX WORDS</th>\n",
       "      <th>FOG INDEX</th>\n",
       "      <th>AVG WORDS PER SENTENCE LENGTH</th>\n",
       "      <th>COMPLEX WORD COUNT</th>\n",
       "      <th>WORD COUNT</th>\n",
       "      <th>SYLLABLE COUNT</th>\n",
       "      <th>PERSONAL PRONOUN COUNT</th>\n",
       "      <th>AVG WORD LENGTH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bctech2011</td>\n",
       "      <td>https://insights.blackcoffer.com/ml-and-ai-bas...</td>\n",
       "      <td>134</td>\n",
       "      <td>44</td>\n",
       "      <td>0.505618</td>\n",
       "      <td>0.076790</td>\n",
       "      <td>13.237288</td>\n",
       "      <td>0.373598</td>\n",
       "      <td>5.444354</td>\n",
       "      <td>13.237288</td>\n",
       "      <td>866</td>\n",
       "      <td>1736</td>\n",
       "      <td>4401</td>\n",
       "      <td>0</td>\n",
       "      <td>7.841052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bctech2012</td>\n",
       "      <td>https://insights.blackcoffer.com/streamlined-i...</td>\n",
       "      <td>25</td>\n",
       "      <td>5</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.060362</td>\n",
       "      <td>9.557692</td>\n",
       "      <td>0.340040</td>\n",
       "      <td>3.959093</td>\n",
       "      <td>9.557692</td>\n",
       "      <td>169</td>\n",
       "      <td>382</td>\n",
       "      <td>902</td>\n",
       "      <td>0</td>\n",
       "      <td>7.824607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bctech2013</td>\n",
       "      <td>https://insights.blackcoffer.com/efficient-dat...</td>\n",
       "      <td>24</td>\n",
       "      <td>9</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.054636</td>\n",
       "      <td>17.257143</td>\n",
       "      <td>0.274834</td>\n",
       "      <td>7.012791</td>\n",
       "      <td>17.257143</td>\n",
       "      <td>166</td>\n",
       "      <td>460</td>\n",
       "      <td>984</td>\n",
       "      <td>0</td>\n",
       "      <td>7.289417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bctech2014</td>\n",
       "      <td>https://insights.blackcoffer.com/effective-man...</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>0.523809</td>\n",
       "      <td>0.042857</td>\n",
       "      <td>9.245283</td>\n",
       "      <td>0.336735</td>\n",
       "      <td>3.832807</td>\n",
       "      <td>9.245283</td>\n",
       "      <td>165</td>\n",
       "      <td>383</td>\n",
       "      <td>851</td>\n",
       "      <td>0</td>\n",
       "      <td>7.516971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bctech2015</td>\n",
       "      <td>https://insights.blackcoffer.com/streamlined-t...</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>0.894737</td>\n",
       "      <td>0.034991</td>\n",
       "      <td>18.724138</td>\n",
       "      <td>0.305709</td>\n",
       "      <td>7.611939</td>\n",
       "      <td>18.724138</td>\n",
       "      <td>166</td>\n",
       "      <td>393</td>\n",
       "      <td>872</td>\n",
       "      <td>0</td>\n",
       "      <td>7.463291</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       URL_ID                                                URL  \\\n",
       "0  bctech2011  https://insights.blackcoffer.com/ml-and-ai-bas...   \n",
       "1  bctech2012  https://insights.blackcoffer.com/streamlined-i...   \n",
       "2  bctech2013  https://insights.blackcoffer.com/efficient-dat...   \n",
       "3  bctech2014  https://insights.blackcoffer.com/effective-man...   \n",
       "4  bctech2015  https://insights.blackcoffer.com/streamlined-t...   \n",
       "\n",
       "   POSITIVE SCORE  NEGATIVE SCORE  POLARITY SCORE  SUBJECTIVITY SCORE  \\\n",
       "0             134              44        0.505618            0.076790   \n",
       "1              25               5        0.666667            0.060362   \n",
       "2              24               9        0.454545            0.054636   \n",
       "3              16               5        0.523809            0.042857   \n",
       "4              18               1        0.894737            0.034991   \n",
       "\n",
       "   AVG SENTENCE LENGTH  PERCENTAGE COMPLEX WORDS  FOG INDEX  \\\n",
       "0            13.237288                  0.373598   5.444354   \n",
       "1             9.557692                  0.340040   3.959093   \n",
       "2            17.257143                  0.274834   7.012791   \n",
       "3             9.245283                  0.336735   3.832807   \n",
       "4            18.724138                  0.305709   7.611939   \n",
       "\n",
       "   AVG WORDS PER SENTENCE LENGTH  COMPLEX WORD COUNT  WORD COUNT  \\\n",
       "0                      13.237288                 866        1736   \n",
       "1                       9.557692                 169         382   \n",
       "2                      17.257143                 166         460   \n",
       "3                       9.245283                 165         383   \n",
       "4                      18.724138                 166         393   \n",
       "\n",
       "   SYLLABLE COUNT  PERSONAL PRONOUN COUNT  AVG WORD LENGTH  \n",
       "0            4401                       0         7.841052  \n",
       "1             902                       0         7.824607  \n",
       "2             984                       0         7.289417  \n",
       "3             851                       0         7.516971  \n",
       "4             872                       0         7.463291  "
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>POSITIVE SCORE</th>\n",
       "      <th>NEGATIVE SCORE</th>\n",
       "      <th>POLARITY SCORE</th>\n",
       "      <th>SUBJECTIVITY SCORE</th>\n",
       "      <th>AVG SENTENCE LENGTH</th>\n",
       "      <th>PERCENTAGE COMPLEX WORDS</th>\n",
       "      <th>FOG INDEX</th>\n",
       "      <th>AVG WORDS PER SENTENCE LENGTH</th>\n",
       "      <th>COMPLEX WORD COUNT</th>\n",
       "      <th>WORD COUNT</th>\n",
       "      <th>SYLLABLE COUNT</th>\n",
       "      <th>PERSONAL PRONOUN COUNT</th>\n",
       "      <th>AVG WORD LENGTH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>147.000000</td>\n",
       "      <td>147.000000</td>\n",
       "      <td>147.000000</td>\n",
       "      <td>147.000000</td>\n",
       "      <td>147.000000</td>\n",
       "      <td>147.000000</td>\n",
       "      <td>147.000000</td>\n",
       "      <td>147.000000</td>\n",
       "      <td>147.000000</td>\n",
       "      <td>147.000000</td>\n",
       "      <td>147.000000</td>\n",
       "      <td>147.000000</td>\n",
       "      <td>147.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>12.639456</td>\n",
       "      <td>4.884354</td>\n",
       "      <td>0.466686</td>\n",
       "      <td>0.043590</td>\n",
       "      <td>24.517210</td>\n",
       "      <td>0.248681</td>\n",
       "      <td>9.906356</td>\n",
       "      <td>24.517616</td>\n",
       "      <td>99.265306</td>\n",
       "      <td>305.755102</td>\n",
       "      <td>620.666667</td>\n",
       "      <td>0.299320</td>\n",
       "      <td>6.860739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>12.905317</td>\n",
       "      <td>5.465354</td>\n",
       "      <td>0.343512</td>\n",
       "      <td>0.018725</td>\n",
       "      <td>20.494290</td>\n",
       "      <td>0.047848</td>\n",
       "      <td>8.197381</td>\n",
       "      <td>20.494156</td>\n",
       "      <td>81.169477</td>\n",
       "      <td>186.523883</td>\n",
       "      <td>433.961812</td>\n",
       "      <td>0.946623</td>\n",
       "      <td>0.365921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>0.006289</td>\n",
       "      <td>8.811321</td>\n",
       "      <td>0.140845</td>\n",
       "      <td>3.659860</td>\n",
       "      <td>8.811321</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>156.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.039666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.233032</td>\n",
       "      <td>0.031100</td>\n",
       "      <td>16.645833</td>\n",
       "      <td>0.218723</td>\n",
       "      <td>6.747206</td>\n",
       "      <td>16.645833</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>196.000000</td>\n",
       "      <td>377.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.633497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>18.906977</td>\n",
       "      <td>0.245318</td>\n",
       "      <td>7.653687</td>\n",
       "      <td>18.906977</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>252.000000</td>\n",
       "      <td>491.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.832117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>15.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.054367</td>\n",
       "      <td>24.131373</td>\n",
       "      <td>0.276203</td>\n",
       "      <td>9.739329</td>\n",
       "      <td>24.131373</td>\n",
       "      <td>121.500000</td>\n",
       "      <td>374.000000</td>\n",
       "      <td>782.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.085431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>134.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.093496</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>71.314607</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>866.000000</td>\n",
       "      <td>1736.000000</td>\n",
       "      <td>4401.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.841052</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       POSITIVE SCORE  NEGATIVE SCORE  POLARITY SCORE  SUBJECTIVITY SCORE  \\\n",
       "count      147.000000      147.000000      147.000000          147.000000   \n",
       "mean        12.639456        4.884354        0.466686            0.043590   \n",
       "std         12.905317        5.465354        0.343512            0.018725   \n",
       "min          2.000000        0.000000       -0.500000            0.006289   \n",
       "25%          6.000000        1.500000        0.233032            0.031100   \n",
       "50%         10.000000        4.000000        0.500000            0.040000   \n",
       "75%         15.000000        6.000000        0.714286            0.054367   \n",
       "max        134.000000       44.000000        1.000000            0.093496   \n",
       "\n",
       "       AVG SENTENCE LENGTH  PERCENTAGE COMPLEX WORDS   FOG INDEX  \\\n",
       "count           147.000000                147.000000  147.000000   \n",
       "mean             24.517210                  0.248681    9.906356   \n",
       "std              20.494290                  0.047848    8.197381   \n",
       "min               8.811321                  0.140845    3.659860   \n",
       "25%              16.645833                  0.218723    6.747206   \n",
       "50%              18.906977                  0.245318    7.653687   \n",
       "75%              24.131373                  0.276203    9.739329   \n",
       "max             178.000000                  0.400000   71.314607   \n",
       "\n",
       "       AVG WORDS PER SENTENCE LENGTH  COMPLEX WORD COUNT   WORD COUNT  \\\n",
       "count                     147.000000          147.000000   147.000000   \n",
       "mean                       24.517616           99.265306   305.755102   \n",
       "std                        20.494156           81.169477   186.523883   \n",
       "min                         8.811321           30.000000    72.000000   \n",
       "25%                        16.645833           55.000000   196.000000   \n",
       "50%                        18.906977           74.000000   252.000000   \n",
       "75%                        24.131373          121.500000   374.000000   \n",
       "max                       178.000000          866.000000  1736.000000   \n",
       "\n",
       "       SYLLABLE COUNT  PERSONAL PRONOUN COUNT  AVG WORD LENGTH  \n",
       "count      147.000000              147.000000       147.000000  \n",
       "mean       620.666667                0.299320         6.860739  \n",
       "std        433.961812                0.946623         0.365921  \n",
       "min        156.000000                0.000000         6.039666  \n",
       "25%        377.500000                0.000000         6.633497  \n",
       "50%        491.000000                0.000000         6.832117  \n",
       "75%        782.500000                0.000000         7.085431  \n",
       "max       4401.000000                7.000000         7.841052  "
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       13.237288\n",
       "1        9.557692\n",
       "2       17.257143\n",
       "3        9.245283\n",
       "4       18.724138\n",
       "          ...    \n",
       "142     20.485714\n",
       "143     15.530303\n",
       "144     17.411765\n",
       "145    178.000000\n",
       "146     16.375000\n",
       "Name: AVG NUMBER OF WORDS PER SENTENCE, Length: 147, dtype: float64"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_df['AVG WORDS PER SENTENCE LENGTH'].rename('AVG NUMBER OF WORDS PER SENTENCE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_df.to_csv('Output Data Structure(Result-Ronit-Mehta).csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('out')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
